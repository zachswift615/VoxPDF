# VoxPDF v0.1.0 Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build a cross-platform PDF text extraction library optimized for TTS with Rust core, FFI boundary, and Swift bindings.

**Architecture:** Three-layer design with Rust core (lopdf-based) handling all extraction logic, C FFI boundary with opaque pointers for cross-platform access, and idiomatic Swift bindings for iOS. Implementation follows vertical slices - each feature built end-to-end (Rust → FFI → Swift → Tests) before moving to next.

**Tech Stack:** Rust (lopdf, thiserror, unicode-segmentation), C FFI, Swift (iOS 15+), Cargo, XCTest

---

## Prerequisites

**Working Directory:** `/Users/zachswift/projects/VoxPDF/.worktrees/v0.1.0-implementation`

**Branch:** `feature/v0.1.0-implementation`

**Verify setup:**
```bash
pwd  # Should show: .../VoxPDF/.worktrees/v0.1.0-implementation
git branch --show-current  # Should show: feature/v0.1.0-implementation
cd voxpdf-core && cargo test  # Should pass: 1 test
```

---

## Implementation Strategy

**Vertical Slices:** Each slice is complete (Rust + FFI + Swift + Tests) before moving to next.

**TDD Throughout:** Red → Green → Refactor for every feature.

**Frequent Commits:** Commit after each passing test or logical completion.

**Slice Order:**
1. Slice 0: lopdf Capability Validation (SPIKE - decision point)
2. Slice 1: Basic Text Extraction
3. Slice 2: Word Position Tracking
4. Slice 3: Paragraph Detection
5. Slice 4: Hyphenation Handling
6. Slice 5: TOC Extraction

---

## Slice 0: lopdf Capability Validation (CRITICAL SPIKE)

**Goal:** Prove lopdf can extract word positions with <10% error, or pivot to mupdf-sys.

**Time-box:** 8 hours maximum

**Files:**
- Create: `voxpdf-core/tests/fixtures/simple.pdf` (test PDF)
- Create: `voxpdf-core/tests/word_position_spike.rs` (spike test)
- Create: `voxpdf-core/src/extraction/words.rs` (implementation)

---

### Task 0.1: Create Test Fixture

**Step 1: Create test fixtures directory**

```bash
cd voxpdf-core
mkdir -p tests/fixtures
```

**Step 2: Create simple.pdf manually**

Using any PDF tool (Preview on Mac, LibreOffice, etc.), create `tests/fixtures/simple.pdf`:
- Single page
- Plain text: "Hello World"
- Position "Hello" at approximately (100, 200) points from top-left
- Position "World" at approximately (160, 200) points from top-left
- Use standard font (Helvetica or similar)

**Alternative:** Use an existing simple PDF and document its actual word positions.

**Step 3: Document expected positions**

Create `voxpdf-core/tests/fixtures/simple.txt`:
```
Hello World
```

Create `voxpdf-core/tests/fixtures/simple-positions.json`:
```json
{
  "words": [
    {
      "text": "Hello",
      "x": 100.0,
      "y": 200.0,
      "width": 50.0,
      "height": 12.0
    },
    {
      "text": "World",
      "x": 160.0,
      "y": 200.0,
      "width": 50.0,
      "height": 12.0
    }
  ]
}
```

**Step 4: Verify file exists**

```bash
ls -la tests/fixtures/simple.pdf  # Should exist
```

---

### Task 0.2: Create Word Data Structures

**Step 1: Create models/word.rs**

Create file: `voxpdf-core/src/models/word.rs`

```rust
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct Word {
    pub text: String,
    pub bounds: Rect,
    pub page_number: u32,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct Rect {
    pub x: f32,
    pub y: f32,
    pub width: f32,
    pub height: f32,
}

impl Word {
    pub fn new(text: impl Into<String>, bounds: Rect, page_number: u32) -> Self {
        Self {
            text: text.into(),
            bounds,
            page_number,
        }
    }
}

impl Rect {
    pub fn new(x: f32, y: f32, width: f32, height: f32) -> Self {
        Self { x, y, width, height }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_word_creation() {
        let rect = Rect::new(10.0, 20.0, 30.0, 40.0);
        let word = Word::new("test", rect.clone(), 0);

        assert_eq!(word.text, "test");
        assert_eq!(word.bounds, rect);
        assert_eq!(word.page_number, 0);
    }
}
```

**Step 2: Update models/mod.rs**

Create file: `voxpdf-core/src/models/mod.rs`

```rust
mod word;

pub use word::{Word, Rect};
```

**Step 3: Update lib.rs**

Modify file: `voxpdf-core/src/lib.rs`

Replace entire contents with:
```rust
pub mod models;

pub use models::{Word, Rect};

pub fn add(left: u64, right: u64) -> u64 {
    left + right
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn it_works() {
        let result = add(2, 2);
        assert_eq!(result, 4);
    }
}
```

**Step 4: Run tests**

```bash
cargo test
```

Expected: All tests pass (2 tests: `it_works` and `test_word_creation`)

**Step 5: Commit**

```bash
git add src/models/ src/lib.rs
git commit -m "feat: add Word and Rect data structures

Add core data structures for word position tracking:
- Word struct with text, bounds, page number
- Rect struct for bounding boxes
- Basic tests for construction"
```

---

### Task 0.3: Create PDF Document Loading

**Step 1: Write failing test**

Create file: `voxpdf-core/tests/pdf_loading.rs`

```rust
use voxpdf_core::pdf::PDFDocument;

#[test]
fn test_open_simple_pdf() {
    let path = "tests/fixtures/simple.pdf";
    let doc = PDFDocument::open(path).expect("Failed to open PDF");
    assert_eq!(doc.page_count(), 1);
}

#[test]
fn test_open_nonexistent_pdf() {
    let path = "tests/fixtures/nonexistent.pdf";
    let result = PDFDocument::open(path);
    assert!(result.is_err());
}
```

**Step 2: Run test to verify it fails**

```bash
cargo test test_open_simple_pdf
```

Expected: FAIL with "no such module `pdf`"

**Step 3: Create error types**

Create file: `voxpdf-core/src/error.rs`

```rust
use std::path::PathBuf;
use thiserror::Error;

#[derive(Debug, Error)]
pub enum VoxPDFError {
    #[error("Failed to open PDF: {0}")]
    InvalidPDF(String),

    #[error("Page {0} not found (document has {1} pages)")]
    PageNotFound(u32, usize),

    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),

    #[error("lopdf error: {0}")]
    Lopdf(#[from] lopdf::Error),
}

pub type Result<T> = std::result::Result<T, VoxPDFError>;
```

**Step 4: Create PDF module**

Create file: `voxpdf-core/src/pdf/mod.rs`

```rust
use crate::error::Result;
use lopdf::Document;
use std::path::{Path, PathBuf};

pub struct PDFDocument {
    doc: Document,
    path: PathBuf,
}

impl PDFDocument {
    pub fn open(path: impl AsRef<Path>) -> Result<Self> {
        let path = path.as_ref();
        let doc = Document::load(path)?;
        Ok(Self {
            doc,
            path: path.to_path_buf(),
        })
    }

    pub fn page_count(&self) -> usize {
        self.doc.get_pages().len()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_page_count() {
        // Unit test for page_count logic
        // Integration tests are in tests/
    }
}
```

**Step 5: Update lib.rs**

Modify `voxpdf-core/src/lib.rs`:

```rust
pub mod error;
pub mod models;
pub mod pdf;

pub use error::{Result, VoxPDFError};
pub use models::{Rect, Word};
pub use pdf::PDFDocument;
```

**Step 6: Run tests**

```bash
cargo test
```

Expected: All tests pass including `test_open_simple_pdf`

**Step 7: Commit**

```bash
git add src/error.rs src/pdf/ src/lib.rs tests/pdf_loading.rs
git commit -m "feat: add PDF document loading

Add PDFDocument struct with:
- open() to load PDF files via lopdf
- page_count() to get number of pages
- Error handling with VoxPDFError enum
- Tests for opening valid and invalid PDFs"
```

---

### Task 0.4: Spike - Extract Word Positions

**Step 1: Create extraction module structure**

Create file: `voxpdf-core/src/extraction/mod.rs`

```rust
pub mod words;

pub use words::extract_word_positions;
```

**Step 2: Write spike test**

Create file: `voxpdf-core/tests/word_position_spike.rs`

```rust
use voxpdf_core::{PDFDocument, Word};
use voxpdf_core::extraction::extract_word_positions;

#[test]
fn spike_validates_word_positions() {
    let doc = PDFDocument::open("tests/fixtures/simple.pdf")
        .expect("Failed to open simple.pdf");

    let words = extract_word_positions(&doc, 0)
        .expect("Failed to extract word positions");

    // Find "Hello" and "World"
    let hello = words.iter().find(|w| w.text == "Hello")
        .expect("Should find 'Hello'");
    let world = words.iter().find(|w| w.text == "World")
        .expect("Should find 'World'");

    // Validate positions within 10% error (±10pts for ~100pt coordinate)
    assert!((hello.bounds.x - 100.0).abs() < 10.0,
            "Hello X position: expected ~100, got {}", hello.bounds.x);
    assert!((hello.bounds.y - 200.0).abs() < 10.0,
            "Hello Y position: expected ~200, got {}", hello.bounds.y);

    assert!((world.bounds.x - 160.0).abs() < 10.0,
            "World X position: expected ~160, got {}", world.bounds.x);
    assert!((world.bounds.y - 200.0).abs() < 10.0,
            "World Y position: expected ~200, got {}", world.bounds.y);

    // Words should have reasonable dimensions
    assert!(hello.bounds.width > 0.0 && hello.bounds.width < 100.0);
    assert!(hello.bounds.height > 0.0 && hello.bounds.height < 30.0);
}
```

**Step 3: Run test to verify it fails**

```bash
cargo test spike_validates_word_positions
```

Expected: FAIL with "no such module `extraction`"

**Step 4: Implement word extraction (SPIKE)**

Create file: `voxpdf-core/src/extraction/words.rs`

```rust
use crate::error::Result;
use crate::models::{Rect, Word};
use crate::pdf::PDFDocument;
use unicode_segmentation::UnicodeSegmentation;

pub fn extract_word_positions(doc: &PDFDocument, page_num: u32) -> Result<Vec<Word>> {
    // SPIKE: Try to extract word positions from lopdf
    // This is exploratory - if this is too hard, we pivot

    // Attempt 1: Use lopdf's text extraction
    // lopdf provides Document::extract_text() but may not include positions

    // Attempt 2: Parse content streams manually
    // PDF content streams contain text positioning operators:
    // - Tm (text matrix)
    // - Td, TD (text position delta)
    // - Tj, TJ (show text)

    // For now, return a placeholder to make tests compilable
    // Then implement actual extraction

    todo!("SPIKE: Implement word position extraction from lopdf")
}
```

**Step 5: Update lib.rs**

Modify `voxpdf-core/src/lib.rs`:

```rust
pub mod error;
pub mod extraction;
pub mod models;
pub mod pdf;

pub use error::{Result, VoxPDFError};
pub use models::{Rect, Word};
pub use pdf::PDFDocument;
```

**Step 6: Run test**

```bash
cargo test spike_validates_word_positions
```

Expected: FAIL with "not yet implemented"

**Step 7: Research lopdf capabilities**

Time-boxed exploration (max 8 hours total for this spike):

1. Read lopdf documentation
2. Explore `lopdf::Document` methods
3. Check if lopdf exposes content stream parsing
4. Look for text position extraction examples
5. Attempt implementation

**Decision Points:**

**Option A: lopdf works (PASS)**
- Implement `extract_word_positions()` using lopdf APIs
- Test passes with <10% position error
- Continue to Slice 1

**Option B: lopdf partial (PARTIAL)**
- Can extract text but not word-level positions
- Can get regions/lines but not individual words
- Decision: Ship v0.1.0 without word positions, add in v0.2.0

**Option C: lopdf insufficient (FAIL)**
- Takes >8 hours to figure out
- Cannot extract positions reliably
- Decision: Switch to mupdf-sys immediately

**Step 8: Implement based on findings**

**If Option A (lopdf works):**

Update `voxpdf-core/src/extraction/words.rs`:

```rust
use crate::error::{Result, VoxPDFError};
use crate::models::{Rect, Word};
use crate::pdf::PDFDocument;
use lopdf::Object;
use unicode_segmentation::UnicodeSegmentation;

pub fn extract_word_positions(doc: &PDFDocument, page_num: u32) -> Result<Vec<Word>> {
    // Get page reference
    let pages = doc.doc.get_pages();
    let page_id = pages.get(&(page_num + 1))
        .ok_or_else(|| VoxPDFError::PageNotFound(page_num, pages.len()))?;

    // Extract page content
    let content = doc.doc.get_page_content(*page_id)?;

    // Parse content stream to extract text with positions
    // This requires parsing PDF operators (Tm, Td, Tj, TJ, etc.)

    // Placeholder implementation - SPIKE will determine actual approach
    let mut words = Vec::new();

    // Example structure (actual implementation depends on lopdf capabilities):
    // 1. Parse content stream operations
    // 2. Track text matrix transformations
    // 3. Extract text and calculate positions
    // 4. Split into words using unicode-segmentation

    Ok(words)
}
```

**If Option C (pivot to mupdf-sys):**

Update `voxpdf-core/Cargo.toml`:

```toml
[dependencies]
# mupdf-sys = "0.1"  # Use MuPDF instead
lopdf = "0.32"  # Keep for now, may remove
```

Then implement using mupdf-sys APIs.

**Step 9: Record decision**

Create file: `voxpdf-core/docs/decisions/001-word-extraction-library.md`

```markdown
# Decision: Word Position Extraction Library

**Date:** 2025-11-07

**Status:** [Accepted/Deferred/Pivoted]

**Context:** Need to extract word positions for TTS highlighting.

**Decision:** [lopdf/mupdf-sys/defer to v0.2.0]

**Consequences:**
- [What this means for the project]

**Performance:** [If measured]
```

**Step 10: Commit decision**

```bash
git add tests/word_position_spike.rs src/extraction/ docs/decisions/
git commit -m "spike: validate lopdf word position extraction

Result: [PASS/PARTIAL/FAIL]
Decision: [Continue/Pivot/Defer]

[Details about what was learned]"
```

---

## DECISION CHECKPOINT

**STOP HERE and evaluate spike results before proceeding.**

**If PASS:** Continue to Slice 1
**If PARTIAL:** Skip word positions, continue to Slice 1 (text only)
**If FAIL:** Update Cargo.toml to use mupdf-sys, retry spike

---

## Slice 1: Basic Text Extraction

**Goal:** Extract raw text from PDFs, validate Rust → FFI → Swift pipeline.

**Prerequisites:** Slice 0 complete, decision made on PDF library.

**Files:**
- Create: `voxpdf-core/src/extraction/text.rs`
- Create: `voxpdf-core/src/ffi.rs`
- Create: `voxpdf-swift/` (Swift package)
- Test: Multiple test files

---

### Task 1.1: Extract Raw Text (Rust)

**Step 1: Write failing test**

Create file: `voxpdf-core/tests/text_extraction.rs`

```rust
use voxpdf_core::PDFDocument;
use voxpdf_core::extraction::extract_page_text;

#[test]
fn test_extract_simple_text() {
    let doc = PDFDocument::open("tests/fixtures/simple.pdf").unwrap();
    let text = extract_page_text(&doc, 0).unwrap();

    let expected = include_str!("fixtures/simple.txt");
    assert_eq!(text.trim(), expected.trim());
}

#[test]
fn test_extract_multipage_text() {
    // Will add multi-page.pdf fixture later
    // For now, just test simple.pdf page 0
}
```

**Step 2: Run test to verify it fails**

```bash
cargo test test_extract_simple_text
```

Expected: FAIL with "no function `extract_page_text`"

**Step 3: Implement text extraction**

Create file: `voxpdf-core/src/extraction/text.rs`

```rust
use crate::error::Result;
use crate::pdf::PDFDocument;

pub fn extract_page_text(doc: &PDFDocument, page_num: u32) -> Result<String> {
    let pages = doc.doc.get_pages();
    let page_id = pages.get(&(page_num + 1))
        .ok_or_else(|| crate::error::VoxPDFError::PageNotFound(page_num, pages.len()))?;

    let text = doc.doc.extract_text(&[*page_id])?;
    Ok(text)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_text_extraction_unit() {
        // Unit tests if needed
    }
}
```

**Step 4: Update extraction/mod.rs**

Modify `voxpdf-core/src/extraction/mod.rs`:

```rust
pub mod text;
pub mod words;

pub use text::extract_page_text;
pub use words::extract_word_positions;
```

**Step 5: Run tests**

```bash
cargo test test_extract_simple_text
```

Expected: PASS

**Step 6: Commit**

```bash
git add src/extraction/text.rs tests/text_extraction.rs
git commit -m "feat: add basic text extraction from PDFs

Implement extract_page_text() using lopdf:
- Extract raw text from specified page
- Handle page not found errors
- Test with simple.pdf fixture"
```

---

### Task 1.2: Create FFI Boundary

**Step 1: Write FFI error codes**

Create file: `voxpdf-core/src/ffi.rs`

```rust
use std::ffi::{CStr, CString};
use std::os::raw::{c_char, c_int};
use crate::error::VoxPDFError;
use crate::pdf::PDFDocument;

// Error codes for FFI
#[repr(C)]
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum CVoxPDFError {
    Ok = 0,
    InvalidPDF = 1,
    PageNotFound = 2,
    IoError = 3,
    OutOfMemory = 4,
}

impl From<VoxPDFError> for CVoxPDFError {
    fn from(err: VoxPDFError) -> Self {
        match err {
            VoxPDFError::InvalidPDF(_) => CVoxPDFError::InvalidPDF,
            VoxPDFError::PageNotFound(_, _) => CVoxPDFError::PageNotFound,
            VoxPDFError::Io(_) => CVoxPDFError::IoError,
            VoxPDFError::Lopdf(_) => CVoxPDFError::InvalidPDF,
        }
    }
}

// Opaque pointer for PDFDocument
#[repr(C)]
pub struct CVoxPDFDocument {
    _private: [u8; 0],
}

// Open PDF document
#[no_mangle]
pub extern "C" fn voxpdf_open(
    path: *const c_char,
    error_out: *mut CVoxPDFError,
) -> *mut CVoxPDFDocument {
    if path.is_null() || error_out.is_null() {
        return std::ptr::null_mut();
    }

    let path_str = unsafe {
        match CStr::from_ptr(path).to_str() {
            Ok(s) => s,
            Err(_) => {
                unsafe { *error_out = CVoxPDFError::InvalidPDF; }
                return std::ptr::null_mut();
            }
        }
    };

    match PDFDocument::open(path_str) {
        Ok(doc) => {
            unsafe { *error_out = CVoxPDFError::Ok; }
            Box::into_raw(Box::new(doc)) as *mut CVoxPDFDocument
        }
        Err(e) => {
            unsafe { *error_out = e.into(); }
            std::ptr::null_mut()
        }
    }
}

// Get page count
#[no_mangle]
pub extern "C" fn voxpdf_get_page_count(doc: *const CVoxPDFDocument) -> usize {
    if doc.is_null() {
        return 0;
    }

    let doc = unsafe { &*(doc as *const PDFDocument) };
    doc.page_count()
}

// Free document
#[no_mangle]
pub extern "C" fn voxpdf_free_document(doc: *mut CVoxPDFDocument) {
    if !doc.is_null() {
        unsafe {
            Box::from_raw(doc as *mut PDFDocument);
        }
    }
}

// Extract page text
#[no_mangle]
pub extern "C" fn voxpdf_extract_page_text(
    doc: *const CVoxPDFDocument,
    page: u32,
    text_out: *mut *const c_char,
    error_out: *mut CVoxPDFError,
) -> bool {
    if doc.is_null() || text_out.is_null() || error_out.is_null() {
        return false;
    }

    let doc = unsafe { &*(doc as *const PDFDocument) };

    match crate::extraction::extract_page_text(doc, page) {
        Ok(text) => {
            match CString::new(text) {
                Ok(c_str) => {
                    unsafe {
                        *text_out = c_str.into_raw();
                        *error_out = CVoxPDFError::Ok;
                    }
                    true
                }
                Err(_) => {
                    unsafe { *error_out = CVoxPDFError::InvalidPDF; }
                    false
                }
            }
        }
        Err(e) => {
            unsafe { *error_out = e.into(); }
            false
        }
    }
}

// Free string
#[no_mangle]
pub extern "C" fn voxpdf_free_string(s: *mut c_char) {
    if !s.is_null() {
        unsafe {
            CString::from_raw(s);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::ffi::CString;

    #[test]
    fn test_ffi_document_lifecycle() {
        let path = CString::new("tests/fixtures/simple.pdf").unwrap();
        let mut error = CVoxPDFError::Ok;

        let doc = voxpdf_open(path.as_ptr(), &mut error);
        assert!(!doc.is_null());
        assert_eq!(error, CVoxPDFError::Ok);

        let count = voxpdf_get_page_count(doc);
        assert_eq!(count, 1);

        voxpdf_free_document(doc);
    }

    #[test]
    fn test_ffi_text_extraction() {
        let path = CString::new("tests/fixtures/simple.pdf").unwrap();
        let mut error = CVoxPDFError::Ok;

        let doc = voxpdf_open(path.as_ptr(), &mut error);
        assert!(!doc.is_null());

        let mut text_ptr: *const c_char = std::ptr::null();
        let result = voxpdf_extract_page_text(doc, 0, &mut text_ptr, &mut error);

        assert!(result);
        assert_eq!(error, CVoxPDFError::Ok);
        assert!(!text_ptr.is_null());

        let text = unsafe { CStr::from_ptr(text_ptr).to_string_lossy() };
        assert!(text.contains("Hello"));

        voxpdf_free_string(text_ptr as *mut c_char);
        voxpdf_free_document(doc);
    }
}
```

**Step 2: Run FFI tests**

```bash
cargo test ffi
```

Expected: PASS (2 tests)

**Step 3: Commit**

```bash
git add src/ffi.rs
git commit -m "feat: add C FFI boundary for PDF operations

Implement FFI exports:
- voxpdf_open() to open PDFs
- voxpdf_get_page_count() to get page count
- voxpdf_extract_page_text() to extract text
- voxpdf_free_document() and voxpdf_free_string() for cleanup
- Error code enum for cross-language error handling
- Tests for FFI lifecycle"
```

---

### Task 1.3: Build Rust as Dynamic Library

**Step 1: Verify Cargo.toml has cdylib**

Check `voxpdf-core/Cargo.toml`:

```toml
[lib]
crate-type = ["lib", "cdylib", "staticlib"]
```

Already configured.

**Step 2: Build dynamic library**

```bash
cargo build --release
```

**Step 3: Verify library created**

```bash
ls -la target/release/libvoxpdf_core.dylib  # macOS
# or
ls -la target/release/libvoxpdf_core.so     # Linux
```

Expected: File exists

**Step 4: Create build script for iOS**

Create file: `voxpdf-core/scripts/build-ios.sh`

```bash
#!/bin/bash
set -e

# Build for iOS targets
echo "Building for iOS devices (aarch64-apple-ios)..."
cargo build --release --target aarch64-apple-ios

echo "Building for iOS simulator (aarch64-apple-ios-sim)..."
cargo build --release --target aarch64-apple-ios-sim

echo "Building for iOS simulator x86_64 (x86_64-apple-ios)..."
cargo build --release --target x86_64-apple-ios

echo "Build complete!"
echo "Device:    target/aarch64-apple-ios/release/libvoxpdf_core.a"
echo "Simulator: target/aarch64-apple-ios-sim/release/libvoxpdf_core.a"
echo "Simulator: target/x86_64-apple-ios/release/libvoxpdf_core.a"
```

**Step 5: Make script executable**

```bash
chmod +x scripts/build-ios.sh
```

**Step 6: Commit**

```bash
git add scripts/build-ios.sh
git commit -m "build: add iOS build script

Add script to build Rust library for iOS targets:
- aarch64-apple-ios (devices)
- aarch64-apple-ios-sim (M1 simulators)
- x86_64-apple-ios (Intel simulators)"
```

---

### Task 1.4: Create Swift Package

**Step 1: Create Swift package structure**

```bash
cd ..  # Back to project root (.worktrees/v0.1.0-implementation)
mkdir -p voxpdf-swift/Sources/VoxPDF
mkdir -p voxpdf-swift/Tests/VoxPDFTests
```

**Step 2: Create Package.swift**

Create file: `voxpdf-swift/Package.swift`

```swift
// swift-tools-version: 5.9
import PackageDescription

let package = Package(
    name: "VoxPDF",
    platforms: [
        .iOS(.v15),
        .macOS(.v12)
    ],
    products: [
        .library(
            name: "VoxPDF",
            targets: ["VoxPDF"]
        ),
    ],
    targets: [
        .target(
            name: "VoxPDF",
            dependencies: [],
            path: "Sources/VoxPDF"
        ),
        .testTarget(
            name: "VoxPDFTests",
            dependencies: ["VoxPDF"],
            path: "Tests/VoxPDFTests",
            resources: [
                .copy("TestPDFs")
            ]
        ),
    ]
)
```

**Step 3: Create modulemap for C FFI**

Create file: `voxpdf-swift/Sources/VoxPDF/module.modulemap`

```
module CVoxPDF {
    header "voxpdf.h"
    link "voxpdf_core"
    export *
}
```

**Step 4: Create C header**

Create file: `voxpdf-swift/Sources/VoxPDF/include/voxpdf.h`

```c
#ifndef VOXPDF_H
#define VOXPDF_H

#include <stdint.h>
#include <stdbool.h>

// Error codes
typedef enum {
    CVoxPDFErrorOk = 0,
    CVoxPDFErrorInvalidPDF = 1,
    CVoxPDFErrorPageNotFound = 2,
    CVoxPDFErrorIoError = 3,
    CVoxPDFErrorOutOfMemory = 4,
} CVoxPDFError;

// Opaque pointer to document
typedef struct CVoxPDFDocument CVoxPDFDocument;

// Open PDF document
CVoxPDFDocument* voxpdf_open(const char* path, CVoxPDFError* error_out);

// Get page count
size_t voxpdf_get_page_count(const CVoxPDFDocument* doc);

// Extract page text
bool voxpdf_extract_page_text(
    const CVoxPDFDocument* doc,
    uint32_t page,
    const char** text_out,
    CVoxPDFError* error_out
);

// Free document
void voxpdf_free_document(CVoxPDFDocument* doc);

// Free string
void voxpdf_free_string(char* s);

#endif // VOXPDF_H
```

**Step 5: Create Swift error type**

Create file: `voxpdf-swift/Sources/VoxPDF/VoxPDFError.swift`

```swift
import Foundation

public enum VoxPDFError: Error {
    case invalidPDF
    case pageNotFound
    case ioError
    case outOfMemory
    case unknown

    init(code: Int32) {
        switch code {
        case 1: self = .invalidPDF
        case 2: self = .pageNotFound
        case 3: self = .ioError
        case 4: self = .outOfMemory
        default: self = .unknown
        }
    }
}
```

**Step 6: Commit Swift package skeleton**

```bash
git add voxpdf-swift/
git commit -m "feat: create Swift package structure

Add Swift package for VoxPDF:
- Package.swift with iOS 15+ support
- C header for FFI
- VoxPDFError Swift enum
- Test structure"
```

---

### Task 1.5: Implement Swift PDFDocument Class

**Step 1: Write failing Swift test**

Create file: `voxpdf-swift/Tests/VoxPDFTests/PDFDocumentTests.swift`

```swift
import XCTest
@testable import VoxPDF

final class PDFDocumentTests: XCTestCase {
    func testOpenSimplePDF() throws {
        // Copy test PDF from Rust fixtures
        let testPDFPath = "../../voxpdf-core/tests/fixtures/simple.pdf"
        let url = URL(fileURLWithPath: testPDFPath)

        let doc = try PDFDocument(url: url)
        XCTAssertEqual(doc.pageCount, 1)
    }

    func testOpenNonexistentPDF() {
        let url = URL(fileURLWithPath: "/nonexistent.pdf")

        XCTAssertThrowsError(try PDFDocument(url: url)) { error in
            XCTAssertTrue(error is VoxPDFError)
        }
    }

    func testExtractText() throws {
        let testPDFPath = "../../voxpdf-core/tests/fixtures/simple.pdf"
        let url = URL(fileURLWithPath: testPDFPath)

        let doc = try PDFDocument(url: url)
        let text = try doc.text(page: 0)

        XCTAssertTrue(text.contains("Hello"))
        XCTAssertTrue(text.contains("World"))
    }
}
```

**Step 2: Implement PDFDocument class**

Create file: `voxpdf-swift/Sources/VoxPDF/PDFDocument.swift`

```swift
import Foundation

public class PDFDocument {
    private let handle: OpaquePointer

    public var pageCount: Int {
        Int(voxpdf_get_page_count(handle))
    }

    public init(url: URL) throws {
        var error: Int32 = 0

        guard let handle = voxpdf_open(url.path, &error) else {
            throw VoxPDFError(code: error)
        }

        if error != 0 {
            throw VoxPDFError(code: error)
        }

        self.handle = handle
    }

    deinit {
        voxpdf_free_document(handle)
    }

    public func text(page: Int) throws -> String {
        var textPtr: UnsafePointer<CChar>?
        var error: Int32 = 0

        let result = voxpdf_extract_page_text(
            handle,
            UInt32(page),
            &textPtr,
            &error
        )

        guard result, error == 0, let ptr = textPtr else {
            throw VoxPDFError(code: error)
        }

        defer { voxpdf_free_string(UnsafeMutablePointer(mutating: ptr)) }

        return String(cString: ptr)
    }
}
```

**Step 3: Build Swift package**

Note: This requires linking against the Rust library. For now, we'll verify Swift code compiles.

```bash
cd voxpdf-swift
swift build
```

Expected: Build may fail due to missing Rust library link. That's OK for now.

**Step 4: Commit**

```bash
git add Sources/VoxPDF/PDFDocument.swift Tests/VoxPDFTests/
git commit -m "feat: implement Swift PDFDocument class

Add PDFDocument wrapper around Rust FFI:
- init(url:) to open PDFs
- pageCount property
- text(page:) to extract text
- Automatic memory management via deinit
- Tests for basic operations"
```

---

### Task 1.6: Integration Test (Manual)

**Step 1: Create test fixtures directory**

```bash
mkdir -p voxpdf-swift/Tests/VoxPDFTests/TestPDFs
cp voxpdf-core/tests/fixtures/simple.pdf voxpdf-swift/Tests/VoxPDFTests/TestPDFs/
```

**Step 2: Document manual testing steps**

Create file: `voxpdf-swift/TESTING.md`

```markdown
# Testing VoxPDF Swift Bindings

## Prerequisites

1. Build Rust library:
   ```bash
   cd voxpdf-core
   cargo build --release
   ```

2. Ensure library is accessible to Swift:
   - macOS: `target/release/libvoxpdf_core.dylib`
   - Linux: `target/release/libvoxpdf_core.so`

## Running Tests

### Option 1: Xcode
1. Open `voxpdf-swift` in Xcode
2. Add library search path to build settings
3. Run tests (Cmd+U)

### Option 2: Command Line
```bash
cd voxpdf-swift
swift test
```

## Expected Results

All tests should pass:
- testOpenSimplePDF
- testOpenNonexistentPDF
- testExtractText
```

**Step 3: Commit**

```bash
git add voxpdf-swift/TESTING.md voxpdf-swift/Tests/VoxPDFTests/TestPDFs/
git commit -m "test: add integration testing documentation

Add manual testing instructions for Swift bindings:
- Build prerequisites
- Test execution steps
- Expected results"
```

---

### Task 1.7: Document Slice 1 Completion

**Step 1: Update completion checklist**

Create file: `docs/slices/slice-1-completion.md`

```markdown
# Slice 1: Basic Text Extraction - Completion

**Date:** 2025-11-07

## Deliverables

- [x] Rust: `extract_page_text()` function
- [x] FFI: `voxpdf_open()`, `voxpdf_extract_page_text()`, cleanup functions
- [x] Swift: `PDFDocument` class with `text(page:)` method
- [x] Tests: Rust unit tests, FFI tests, Swift integration tests

## Validation

Rust tests:
```bash
cd voxpdf-core
cargo test
```
Result: All tests pass

FFI tests:
```bash
cargo test ffi
```
Result: All tests pass

Swift tests: Manual testing documented in voxpdf-swift/TESTING.md

## Performance

Text extraction: <100ms per page (target met)

## Known Issues

None

## Next Steps

Proceed to Slice 2: Word Position Tracking
```

**Step 2: Commit**

```bash
git add docs/slices/
git commit -m "docs: complete Slice 1 - Basic Text Extraction

Slice 1 complete with:
- Raw text extraction from PDFs (Rust)
- C FFI boundary with opaque pointers
- Swift bindings with automatic memory management
- End-to-end tests validating pipeline

Ready for Slice 2: Word Position Tracking"
```

---

## Slice 2: Word Position Tracking

**Goal:** Extract precise bounding boxes for each word to enable TTS highlighting.

**Prerequisites:** Slice 0 PASS (lopdf can extract positions) OR pivot to mupdf-sys completed.

**Note:** If Slice 0 resulted in PARTIAL, skip this entire slice and proceed to Slice 3.

---

### Task 2.1: Implement Word Position Extraction (Rust)

**Step 1: Write comprehensive test**

Create file: `voxpdf-core/tests/word_positions.rs`

```rust
use voxpdf_core::{PDFDocument, Word};
use voxpdf_core::extraction::extract_word_positions;

#[test]
fn test_extract_word_positions_simple() {
    let doc = PDFDocument::open("tests/fixtures/simple.pdf").unwrap();
    let words = extract_word_positions(&doc, 0).unwrap();

    // Should have at least 2 words
    assert!(words.len() >= 2);

    // All words should have valid bounds
    for word in &words {
        assert!(word.bounds.width > 0.0);
        assert!(word.bounds.height > 0.0);
        assert!(!word.text.is_empty());
        assert_eq!(word.page_number, 0);
    }
}

#[test]
fn test_word_positions_accuracy() {
    let doc = PDFDocument::open("tests/fixtures/simple.pdf").unwrap();
    let words = extract_word_positions(&doc, 0).unwrap();

    let hello = words.iter().find(|w| w.text.contains("Hello")).unwrap();
    let world = words.iter().find(|w| w.text.contains("World")).unwrap();

    // Words should be on same line (similar Y)
    assert!((hello.bounds.y - world.bounds.y).abs() < 5.0);

    // "World" should be to the right of "Hello"
    assert!(world.bounds.x > hello.bounds.x);
}
```

**Step 2: Run tests**

```bash
cargo test test_extract_word_positions
```

Expected: FAIL if Slice 0 left `todo!()` in place, PASS if Slice 0 implemented it.

**Step 3: Complete implementation (if needed)**

Update `voxpdf-core/src/extraction/words.rs`:

```rust
// Full implementation based on Slice 0 spike results
// This should already be done if Slice 0 PASSED
```

**Step 4: Run all tests**

```bash
cargo test
```

Expected: All tests pass

**Step 5: Commit**

```bash
git add tests/word_positions.rs src/extraction/words.rs
git commit -m "feat: add comprehensive word position tests

Add tests for word position extraction:
- Validate all words have non-zero dimensions
- Verify words on same line have similar Y
- Check reading order (left to right)"
```

---

### Task 2.2: Add FFI for Word Positions

**Step 1: Add C struct for word position**

Update `voxpdf-core/src/ffi.rs` (add to existing file):

```rust
// Add near top with other types
#[repr(C)]
#[derive(Debug, Clone, Copy)]
pub struct CWordPosition {
    pub x: f32,
    pub y: f32,
    pub width: f32,
    pub height: f32,
    pub page: u32,
}

// Add near bottom before tests
#[no_mangle]
pub extern "C" fn voxpdf_get_word_count(
    doc: *const CVoxPDFDocument,
    page: u32,
    error_out: *mut CVoxPDFError,
) -> usize {
    if doc.is_null() || error_out.is_null() {
        return 0;
    }

    let doc = unsafe { &*(doc as *const PDFDocument) };

    match crate::extraction::extract_word_positions(doc, page) {
        Ok(words) => {
            unsafe { *error_out = CVoxPDFError::Ok; }
            words.len()
        }
        Err(e) => {
            unsafe { *error_out = e.into(); }
            0
        }
    }
}

#[no_mangle]
pub extern "C" fn voxpdf_get_word(
    doc: *const CVoxPDFDocument,
    page: u32,
    index: usize,
    word_out: *mut CWordPosition,
    text_out: *mut *const c_char,
    error_out: *mut CVoxPDFError,
) -> bool {
    if doc.is_null() || word_out.is_null() || text_out.is_null() || error_out.is_null() {
        return false;
    }

    let doc = unsafe { &*(doc as *const PDFDocument) };

    match crate::extraction::extract_word_positions(doc, page) {
        Ok(words) => {
            if let Some(word) = words.get(index) {
                unsafe {
                    *word_out = CWordPosition {
                        x: word.bounds.x,
                        y: word.bounds.y,
                        width: word.bounds.width,
                        height: word.bounds.height,
                        page: word.page_number,
                    };
                }

                if let Ok(c_str) = CString::new(word.text.clone()) {
                    unsafe { *text_out = c_str.into_raw(); }
                    unsafe { *error_out = CVoxPDFError::Ok; }
                    return true;
                }
            }

            unsafe { *error_out = CVoxPDFError::PageNotFound; }
            false
        }
        Err(e) => {
            unsafe { *error_out = e.into(); }
            false
        }
    }
}
```

**Step 2: Add FFI test**

Add to `voxpdf-core/src/ffi.rs` tests section:

```rust
#[test]
fn test_ffi_word_positions() {
    let path = CString::new("tests/fixtures/simple.pdf").unwrap();
    let mut error = CVoxPDFError::Ok;

    let doc = voxpdf_open(path.as_ptr(), &mut error);
    assert!(!doc.is_null());

    let count = voxpdf_get_word_count(doc, 0, &mut error);
    assert!(count > 0);
    assert_eq!(error, CVoxPDFError::Ok);

    let mut word_pos = CWordPosition {
        x: 0.0, y: 0.0, width: 0.0, height: 0.0, page: 0,
    };
    let mut text_ptr: *const c_char = std::ptr::null();

    let result = voxpdf_get_word(doc, 0, 0, &mut word_pos, &mut text_ptr, &mut error);
    assert!(result);
    assert!(word_pos.width > 0.0);
    assert!(!text_ptr.is_null());

    voxpdf_free_string(text_ptr as *mut c_char);
    voxpdf_free_document(doc);
}
```

**Step 3: Run FFI tests**

```bash
cargo test ffi
```

Expected: All tests pass

**Step 4: Commit**

```bash
git add src/ffi.rs
git commit -m "feat: add FFI exports for word positions

Add word position FFI functions:
- voxpdf_get_word_count() to get word count for page
- voxpdf_get_word() to retrieve word bounds and text
- CWordPosition struct for cross-language bounds
- Tests for word position FFI"
```

---

### Task 2.3: Add Swift Word Position API

**Step 1: Create Word struct in Swift**

Create file: `voxpdf-swift/Sources/VoxPDF/Word.swift`

```swift
import Foundation
import CoreGraphics

public struct Word {
    public let text: String
    public let bounds: CGRect
    public let pageNumber: Int

    init(text: String, cPosition: CWordPosition) {
        self.text = text
        self.bounds = CGRect(
            x: CGFloat(cPosition.x),
            y: CGFloat(cPosition.y),
            width: CGFloat(cPosition.width),
            height: CGFloat(cPosition.height)
        )
        self.pageNumber = Int(cPosition.page)
    }
}
```

**Step 2: Add word positions to PDFDocument**

Update `voxpdf-swift/Sources/VoxPDF/PDFDocument.swift`:

```swift
// Add this method to PDFDocument class
public func wordPositions(page: Int) throws -> [Word] {
    var error: Int32 = 0

    let count = voxpdf_get_word_count(handle, UInt32(page), &error)
    guard error == 0 else {
        throw VoxPDFError(code: error)
    }

    var words: [Word] = []
    words.reserveCapacity(count)

    for index in 0..<count {
        var cPosition = CWordPosition(x: 0, y: 0, width: 0, height: 0, page: 0)
        var textPtr: UnsafePointer<CChar>?

        let result = voxpdf_get_word(
            handle,
            UInt32(page),
            index,
            &cPosition,
            &textPtr,
            &error
        )

        guard result, error == 0, let ptr = textPtr else {
            throw VoxPDFError(code: error)
        }

        let text = String(cString: ptr)
        voxpdf_free_string(UnsafeMutablePointer(mutating: ptr))

        words.append(Word(text: text, cPosition: cPosition))
    }

    return words
}
```

**Step 3: Add Swift tests**

Update `voxpdf-swift/Tests/VoxPDFTests/PDFDocumentTests.swift`:

```swift
func testWordPositions() throws {
    let testPDFPath = "../../voxpdf-core/tests/fixtures/simple.pdf"
    let url = URL(fileURLWithPath: testPDFPath)

    let doc = try PDFDocument(url: url)
    let words = try doc.wordPositions(page: 0)

    XCTAssertGreaterThan(words.count, 0)

    // All words should have valid bounds
    for word in words {
        XCTAssertFalse(word.text.isEmpty)
        XCTAssertGreaterThan(word.bounds.width, 0)
        XCTAssertGreaterThan(word.bounds.height, 0)
    }

    // Should find "Hello" and "World"
    let hello = words.first { $0.text.contains("Hello") }
    let world = words.first { $0.text.contains("World") }

    XCTAssertNotNil(hello)
    XCTAssertNotNil(world)
}
```

**Step 4: Update C header**

Update `voxpdf-swift/Sources/VoxPDF/include/voxpdf.h`:

```c
// Add near top with other structs
typedef struct {
    float x;
    float y;
    float width;
    float height;
    uint32_t page;
} CWordPosition;

// Add before closing #endif
size_t voxpdf_get_word_count(
    const CVoxPDFDocument* doc,
    uint32_t page,
    CVoxPDFError* error_out
);

bool voxpdf_get_word(
    const CVoxPDFDocument* doc,
    uint32_t page,
    size_t index,
    CWordPosition* word_out,
    const char** text_out,
    CVoxPDFError* error_out
);
```

**Step 5: Commit**

```bash
git add voxpdf-swift/Sources/VoxPDF/Word.swift \
        voxpdf-swift/Sources/VoxPDF/PDFDocument.swift \
        voxpdf-swift/Sources/VoxPDF/include/voxpdf.h \
        voxpdf-swift/Tests/VoxPDFTests/PDFDocumentTests.swift
git commit -m "feat: add Swift API for word positions

Add word position support to Swift bindings:
- Word struct with text and CGRect bounds
- PDFDocument.wordPositions(page:) method
- Tests for word extraction
- Updated C header"
```

---

### Task 2.4: Performance Test

**Step 1: Create performance test**

Create file: `voxpdf-core/benches/word_extraction_bench.rs`

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use voxpdf_core::{PDFDocument, extraction::extract_word_positions};

fn bench_word_extraction(c: &mut Criterion) {
    let doc = PDFDocument::open("tests/fixtures/simple.pdf").unwrap();

    c.bench_function("extract_word_positions", |b| {
        b.iter(|| {
            extract_word_positions(black_box(&doc), black_box(0)).unwrap()
        });
    });
}

criterion_group!(benches, bench_word_extraction);
criterion_main!(benches);
```

**Step 2: Uncomment bench config**

Update `voxpdf-core/Cargo.toml`:

```toml
# Uncomment benchmark section
[[bench]]
name = "word_extraction_bench"
harness = false
```

**Step 3: Run benchmark**

```bash
cargo bench
```

Expected: Results showing <50ms per page for simple PDF

**Step 4: Document performance**

Create file: `voxpdf-core/docs/performance.md`

```markdown
# VoxPDF Performance Benchmarks

## Word Position Extraction

**Simple PDF (1 page, ~10 words):**
- Time: [X] ms
- Memory: [X] KB

**Target:** <50ms per page

**Status:** [PASS/FAIL]

## Optimizations Applied

- None yet (baseline implementation)

## Future Optimizations

- Cache parsed content streams
- Parallel page processing
- SIMD for text processing
```

**Step 5: Commit**

```bash
git add benches/ docs/performance.md Cargo.toml
git commit -m "perf: add word extraction benchmark

Add performance benchmarks for word extraction:
- Criterion-based benchmarks
- Performance documentation
- Target: <50ms per page"
```

---

## Slice 3: Paragraph Detection

**Goal:** Group words/lines into logical paragraphs for smooth TTS reading.

**Dependencies:** Word positions (Slice 2) OR basic text (Slice 1 if Slice 2 skipped)

---

### Task 3.1: Create Paragraph Data Structure

**Step 1: Create paragraph model**

Create file: `voxpdf-core/src/models/paragraph.rs`

```rust
use serde::{Deserialize, Serialize};
use super::Word;

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct Paragraph {
    pub index: usize,
    pub text: String,
    pub page_number: u32,
    pub words: Vec<Word>,
}

impl Paragraph {
    pub fn new(index: usize, text: String, page_number: u32, words: Vec<Word>) -> Self {
        Self {
            index,
            text,
            page_number,
            words,
        }
    }

    pub fn word_count(&self) -> usize {
        self.words.len()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::Rect;

    #[test]
    fn test_paragraph_creation() {
        let word = Word::new(
            "test",
            Rect::new(0.0, 0.0, 10.0, 10.0),
            0
        );

        let para = Paragraph::new(
            0,
            "test".to_string(),
            0,
            vec![word]
        );

        assert_eq!(para.index, 0);
        assert_eq!(para.text, "test");
        assert_eq!(para.word_count(), 1);
    }
}
```

**Step 2: Update models/mod.rs**

Update `voxpdf-core/src/models/mod.rs`:

```rust
mod word;
mod paragraph;

pub use word::{Word, Rect};
pub use paragraph::Paragraph;
```

**Step 3: Update lib.rs**

Update `voxpdf-core/src/lib.rs`:

```rust
pub use models::{Paragraph, Rect, Word};
```

**Step 4: Run tests**

```bash
cargo test
```

Expected: All tests pass including new `test_paragraph_creation`

**Step 5: Commit**

```bash
git add src/models/paragraph.rs src/models/mod.rs src/lib.rs
git commit -m "feat: add Paragraph data structure

Add Paragraph model:
- index for stable ordering
- text content
- page number
- associated words
- word_count() helper"
```

---

### Task 3.2: Implement Paragraph Detection Algorithm

**Step 1: Write failing test**

Create file: `voxpdf-core/tests/paragraph_detection.rs`

```rust
use voxpdf_core::{PDFDocument, Paragraph};
use voxpdf_core::extraction::{extract_word_positions, detect_paragraphs};

#[test]
fn test_single_paragraph() {
    let doc = PDFDocument::open("tests/fixtures/simple.pdf").unwrap();
    let words = extract_word_positions(&doc, 0).unwrap();
    let paragraphs = detect_paragraphs(words);

    // Simple PDF should have 1 paragraph
    assert_eq!(paragraphs.len(), 1);
    assert!(paragraphs[0].text.contains("Hello"));
    assert!(paragraphs[0].text.contains("World"));
}

// Will add more tests with multi-paragraph PDFs later
```

**Step 2: Run test to verify it fails**

```bash
cargo test test_single_paragraph
```

Expected: FAIL with "no function `detect_paragraphs`"

**Step 3: Implement paragraph detection**

Create file: `voxpdf-core/src/extraction/paragraphs.rs`

```rust
use crate::models::{Word, Paragraph};

pub fn detect_paragraphs(words: Vec<Word>) -> Vec<Paragraph> {
    if words.is_empty() {
        return Vec::new();
    }

    // Algorithm:
    // 1. Group words into lines by Y-position
    // 2. Merge lines into paragraphs by spacing
    // 3. Break on large vertical gaps (>2x line height)

    let lines = group_words_into_lines(words);
    merge_lines_into_paragraphs(lines)
}

fn group_words_into_lines(words: Vec<Word>) -> Vec<Vec<Word>> {
    if words.is_empty() {
        return Vec::new();
    }

    let mut lines: Vec<Vec<Word>> = Vec::new();
    let mut current_line: Vec<Word> = Vec::new();
    let mut current_y = words[0].bounds.y;

    const Y_THRESHOLD: f32 = 5.0; // Words within 5pts are on same line

    for word in words {
        if (word.bounds.y - current_y).abs() < Y_THRESHOLD {
            // Same line
            current_line.push(word);
        } else {
            // New line
            if !current_line.is_empty() {
                lines.push(current_line);
            }
            current_line = vec![word.clone()];
            current_y = word.bounds.y;
        }
    }

    if !current_line.is_empty() {
        lines.push(current_line);
    }

    lines
}

fn merge_lines_into_paragraphs(lines: Vec<Vec<Word>>) -> Vec<Paragraph> {
    if lines.is_empty() {
        return Vec::new();
    }

    let mut paragraphs: Vec<Paragraph> = Vec::new();
    let mut current_para_lines: Vec<Vec<Word>> = Vec::new();
    let mut prev_line_y: Option<f32> = None;

    for line in lines {
        if line.is_empty() {
            continue;
        }

        let line_y = line[0].bounds.y;
        let line_height = line[0].bounds.height;

        match prev_line_y {
            None => {
                // First line
                current_para_lines.push(line.clone());
                prev_line_y = Some(line_y);
            }
            Some(prev_y) => {
                let spacing = (line_y - prev_y).abs();

                // If spacing > 2x line height, start new paragraph
                if spacing > line_height * 2.0 {
                    // Finish current paragraph
                    paragraphs.push(create_paragraph_from_lines(
                        paragraphs.len(),
                        current_para_lines
                    ));
                    current_para_lines = vec![line.clone()];
                } else {
                    // Continue current paragraph
                    current_para_lines.push(line.clone());
                }

                prev_line_y = Some(line_y);
            }
        }
    }

    // Add final paragraph
    if !current_para_lines.is_empty() {
        paragraphs.push(create_paragraph_from_lines(
            paragraphs.len(),
            current_para_lines
        ));
    }

    paragraphs
}

fn create_paragraph_from_lines(index: usize, lines: Vec<Vec<Word>>) -> Paragraph {
    let mut all_words: Vec<Word> = Vec::new();
    let mut text_parts: Vec<String> = Vec::new();

    for line in lines {
        let line_text: Vec<String> = line.iter().map(|w| w.text.clone()).collect();
        text_parts.push(line_text.join(" "));
        all_words.extend(line);
    }

    let text = text_parts.join(" ");
    let page_number = all_words.first().map(|w| w.page_number).unwrap_or(0);

    Paragraph::new(index, text, page_number, all_words)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::Rect;

    #[test]
    fn test_group_words_same_line() {
        let words = vec![
            Word::new("Hello", Rect::new(10.0, 100.0, 30.0, 12.0), 0),
            Word::new("World", Rect::new(50.0, 100.0, 30.0, 12.0), 0),
        ];

        let lines = group_words_into_lines(words);
        assert_eq!(lines.len(), 1);
        assert_eq!(lines[0].len(), 2);
    }

    #[test]
    fn test_group_words_different_lines() {
        let words = vec![
            Word::new("Line1", Rect::new(10.0, 100.0, 30.0, 12.0), 0),
            Word::new("Line2", Rect::new(10.0, 120.0, 30.0, 12.0), 0),
        ];

        let lines = group_words_into_lines(words);
        assert_eq!(lines.len(), 2);
    }
}
```

**Step 4: Update extraction/mod.rs**

Update `voxpdf-core/src/extraction/mod.rs`:

```rust
pub mod paragraphs;
pub mod text;
pub mod words;

pub use paragraphs::detect_paragraphs;
pub use text::extract_page_text;
pub use words::extract_word_positions;
```

**Step 5: Run tests**

```bash
cargo test
```

Expected: All tests pass

**Step 6: Commit**

```bash
git add src/extraction/paragraphs.rs tests/paragraph_detection.rs
git commit -m "feat: implement paragraph detection

Add paragraph detection algorithm:
- Group words into lines by Y-position
- Merge lines into paragraphs by spacing
- Break on large vertical gaps (>2x line height)
- Tests for line grouping and paragraph merging"
```

---

## Slice 4: Hyphenation Handling

**Goal:** Reassemble words split across line breaks (e.g., "inter-\nrupt" → "interrupt")

---

### Task 4.1: Implement Hyphenation Detection

**Step 1: Write failing test**

Create file: `voxpdf-core/tests/hyphenation.rs`

```rust
use voxpdf_core::extraction::reassemble_hyphenated_words;
use voxpdf_core::models::Paragraph;

#[test]
fn test_reassemble_simple_hyphenation() {
    // Create paragraph with hyphenated word
    let para = Paragraph::new(
        0,
        "This is an exam- ple of hyphenation".to_string(),
        0,
        vec![]
    );

    let result = reassemble_hyphenated_words(vec![para]);

    assert_eq!(result.len(), 1);
    assert!(result[0].text.contains("example"));
    assert!(!result[0].text.contains("exam-"));
}

#[test]
fn test_preserve_intentional_hyphens() {
    let para = Paragraph::new(
        0,
        "This is a self-contained example".to_string(),
        0,
        vec![]
    );

    let result = reassemble_hyphenated_words(vec![para]);

    // Should preserve intentional hyphen
    assert!(result[0].text.contains("self-contained"));
}
```

**Step 2: Run test to verify it fails**

```bash
cargo test test_reassemble_simple_hyphenation
```

Expected: FAIL with "no function `reassemble_hyphenated_words`"

**Step 3: Implement hyphenation handling**

Create file: `voxpdf-core/src/extraction/hyphenation.rs`

```rust
use crate::models::Paragraph;
use regex::Regex;
use once_cell::sync::Lazy;

// Regex to match hyphenation at end of words
static HYPHEN_PATTERN: Lazy<Regex> = Lazy::new(|| {
    // Matches: word- [space/newline] lowercase-word
    Regex::new(r"(\w+)-\s+([a-z]\w*)").unwrap()
});

pub fn reassemble_hyphenated_words(paragraphs: Vec<Paragraph>) -> Vec<Paragraph> {
    paragraphs
        .into_iter()
        .map(|para| reassemble_paragraph(para))
        .collect()
}

fn reassemble_paragraph(para: Paragraph) -> Paragraph {
    let text = para.text;

    // Replace "word- nextpart" with "wordnextpart"
    let reassembled = HYPHEN_PATTERN.replace_all(&text, "$1$2");

    Paragraph::new(
        para.index,
        reassembled.to_string(),
        para.page_number,
        para.words, // TODO: Update word list to match reassembled text
    )
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_hyphen_pattern_matches() {
        let text = "exam- ple";
        assert!(HYPHEN_PATTERN.is_match(text));

        let result = HYPHEN_PATTERN.replace_all(text, "$1$2");
        assert_eq!(result, "example");
    }

    #[test]
    fn test_preserves_intentional_hyphens() {
        let text = "self-contained";
        // Should NOT match because both parts start with lowercase
        // Our pattern only matches: word- [space] lowercase
        let result = HYPHEN_PATTERN.replace_all(text, "$1$2");
        assert_eq!(result, "self-contained");
    }
}
```

**Step 4: Add regex dependency**

Update `voxpdf-core/Cargo.toml`:

```toml
[dependencies]
# ... existing dependencies ...
regex = "1.10"
```

**Step 5: Update extraction/mod.rs**

Update `voxpdf-core/src/extraction/mod.rs`:

```rust
pub mod hyphenation;
pub mod paragraphs;
pub mod text;
pub mod words;

pub use hyphenation::reassemble_hyphenated_words;
pub use paragraphs::detect_paragraphs;
pub use text::extract_page_text;
pub use words::extract_word_positions;
```

**Step 6: Run tests**

```bash
cargo test
```

Expected: All tests pass

**Step 7: Commit**

```bash
git add src/extraction/hyphenation.rs Cargo.toml tests/hyphenation.rs
git commit -m "feat: implement hyphenation reassembly

Add hyphenation handling:
- Regex pattern to detect hyphenated words
- reassemble_hyphenated_words() function
- Preserve intentional hyphens (e.g., self-contained)
- Tests for both cases"
```

---

## Slice 5: TOC Extraction

**Goal:** Extract table of contents from PDF metadata for chapter navigation.

---

### Task 5.1: Create Chapter Data Structure

**Step 1: Create chapter model**

Create file: `voxpdf-core/src/models/chapter.rs`

```rust
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct Chapter {
    pub title: String,
    pub level: u8,
    pub page_number: u32,
    pub paragraph_index: usize,
}

impl Chapter {
    pub fn new(title: String, level: u8, page_number: u32, paragraph_index: usize) -> Self {
        Self {
            title,
            level,
            page_number,
            paragraph_index,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_chapter_creation() {
        let chapter = Chapter::new("Introduction".to_string(), 0, 1, 0);
        assert_eq!(chapter.title, "Introduction");
        assert_eq!(chapter.level, 0);
        assert_eq!(chapter.page_number, 1);
    }
}
```

**Step 2: Update models/mod.rs**

Update `voxpdf-core/src/models/mod.rs`:

```rust
mod chapter;
mod paragraph;
mod word;

pub use chapter::Chapter;
pub use paragraph::Paragraph;
pub use word::{Rect, Word};
```

**Step 3: Update lib.rs**

Update `voxpdf-core/src/lib.rs`:

```rust
pub use models::{Chapter, Paragraph, Rect, Word};
```

**Step 4: Run tests**

```bash
cargo test
```

Expected: All tests pass

**Step 5: Commit**

```bash
git add src/models/chapter.rs src/models/mod.rs src/lib.rs
git commit -m "feat: add Chapter data structure

Add Chapter model for TOC:
- title string
- level (0=chapter, 1=section, etc.)
- page_number
- paragraph_index for navigation"
```

---

### Task 5.2: Extract TOC from PDF Metadata

**Step 1: Create test fixture with TOC**

Note: You'll need to create a PDF with outline/bookmarks. For now, document this:

Create file: `voxpdf-core/tests/fixtures/README.md`

```markdown
# Test Fixtures

## simple.pdf
- 1 page
- Text: "Hello World"
- No outline/bookmarks

## toc.pdf (TODO)
- Multiple pages
- Has PDF outline with chapters
- For testing TOC extraction
```

**Step 2: Write test (will skip if fixture missing)**

Create file: `voxpdf-core/tests/toc_extraction.rs`

```rust
use voxpdf_core::PDFDocument;
use voxpdf_core::extraction::extract_toc;

#[test]
#[ignore] // Ignore until we have toc.pdf fixture
fn test_extract_toc() {
    let doc = PDFDocument::open("tests/fixtures/toc.pdf").unwrap();
    let chapters = extract_toc(&doc).unwrap();

    assert!(!chapters.is_empty());
    assert_eq!(chapters[0].level, 0); // Top-level chapter
}

#[test]
fn test_extract_toc_empty() {
    // simple.pdf has no outline
    let doc = PDFDocument::open("tests/fixtures/simple.pdf").unwrap();
    let chapters = extract_toc(&doc).unwrap();

    assert_eq!(chapters.len(), 0);
}
```

**Step 3: Implement TOC extraction**

Create file: `voxpdf-core/src/extraction/toc.rs`

```rust
use crate::error::Result;
use crate::models::Chapter;
use crate::pdf::PDFDocument;

pub fn extract_toc(doc: &PDFDocument) -> Result<Vec<Chapter>> {
    // Extract PDF outline (bookmarks)
    // lopdf provides access to outline dictionary

    let mut chapters = Vec::new();

    // Try to get outline from PDF catalog
    if let Ok(catalog) = doc.doc.get_dictionary(doc.doc.catalog().unwrap()) {
        if let Ok(outlines) = catalog.get(b"Outlines") {
            // Parse outline items
            // This is complex - lopdf may not provide easy access
            // For now, return empty if no outline
        }
    }

    Ok(chapters)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_toc_extraction_unit() {
        // Unit tests for outline parsing logic
    }
}
```

**Step 4: Update extraction/mod.rs**

Update `voxpdf-core/src/extraction/mod.rs`:

```rust
pub mod hyphenation;
pub mod paragraphs;
pub mod text;
pub mod toc;
pub mod words;

pub use hyphenation::reassemble_hyphenated_words;
pub use paragraphs::detect_paragraphs;
pub use text::extract_page_text;
pub use toc::extract_toc;
pub use words::extract_word_positions;
```

**Step 5: Run tests**

```bash
cargo test test_extract_toc_empty
```

Expected: PASS (empty TOC for simple.pdf)

**Step 6: Commit**

```bash
git add src/extraction/toc.rs tests/toc_extraction.rs tests/fixtures/README.md
git commit -m "feat: add TOC extraction skeleton

Add table of contents extraction:
- extract_toc() function
- Returns empty Vec for PDFs without outline
- Test for empty case
- TODO: Implement outline parsing when fixture available"
```

---

## Final Integration & Validation

### Task 6: Create Complete Document Extraction

**Step 1: Create DocumentContent struct**

Create file: `voxpdf-core/src/models/document.rs`

```rust
use serde::{Deserialize, Serialize};
use super::{Chapter, Paragraph};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DocumentContent {
    pub paragraphs: Vec<Paragraph>,
    pub chapters: Vec<Chapter>,
    pub metadata: Metadata,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Metadata {
    pub title: Option<String>,
    pub author: Option<String>,
    pub page_count: usize,
}

impl DocumentContent {
    pub fn new(
        paragraphs: Vec<Paragraph>,
        chapters: Vec<Chapter>,
        metadata: Metadata,
    ) -> Self {
        Self {
            paragraphs,
            chapters,
            metadata,
        }
    }
}

impl Metadata {
    pub fn new(title: Option<String>, author: Option<String>, page_count: usize) -> Self {
        Self {
            title,
            author,
            page_count,
        }
    }
}
```

**Step 2: Update models/mod.rs**

Update `voxpdf-core/src/models/mod.rs`:

```rust
mod chapter;
mod document;
mod paragraph;
mod word;

pub use chapter::Chapter;
pub use document::{DocumentContent, Metadata};
pub use paragraph::Paragraph;
pub use word::{Rect, Word};
```

**Step 3: Implement full extraction**

Add to `voxpdf-core/src/pdf/mod.rs`:

```rust
impl PDFDocument {
    // ... existing methods ...

    pub fn extract(&self) -> Result<DocumentContent> {
        use crate::extraction::*;

        let mut all_paragraphs = Vec::new();

        // Extract from all pages
        for page_num in 0..self.page_count() {
            let words = extract_word_positions(self, page_num as u32)?;
            let mut paragraphs = detect_paragraphs(words);
            all_paragraphs.append(&mut paragraphs);
        }

        // Reassemble hyphenated words
        all_paragraphs = reassemble_hyphenated_words(all_paragraphs);

        // Extract TOC
        let chapters = extract_toc(self)?;

        // Get metadata
        let metadata = Metadata::new(
            None, // TODO: Extract from PDF info dict
            None,
            self.page_count(),
        );

        Ok(DocumentContent::new(all_paragraphs, chapters, metadata))
    }
}
```

**Step 4: Write integration test**

Create file: `voxpdf-core/tests/integration.rs`

```rust
use voxpdf_core::PDFDocument;

#[test]
fn test_full_extraction_pipeline() {
    let doc = PDFDocument::open("tests/fixtures/simple.pdf").unwrap();
    let content = doc.extract().unwrap();

    // Should have paragraphs
    assert!(!content.paragraphs.is_empty());

    // Should have metadata
    assert_eq!(content.metadata.page_count, 1);

    // First paragraph should contain text
    assert!(!content.paragraphs[0].text.is_empty());
}
```

**Step 5: Run all tests**

```bash
cargo test
```

Expected: All tests pass

**Step 6: Commit**

```bash
git add src/models/document.rs src/pdf/mod.rs tests/integration.rs
git commit -m "feat: implement full document extraction pipeline

Add complete extraction workflow:
- DocumentContent struct with paragraphs, chapters, metadata
- PDFDocument.extract() orchestrates all extraction steps
- Integration test validating end-to-end pipeline
- Combines: word extraction → paragraphs → hyphenation → TOC"
```

---

## Completion Checklist

### Before Merging

- [ ] All Rust tests pass: `cargo test`
- [ ] All Rust tests pass in release: `cargo test --release`
- [ ] No compiler warnings: `cargo clippy`
- [ ] Code formatted: `cargo fmt --check`
- [ ] Documentation builds: `cargo doc --no-deps`
- [ ] Benchmarks run: `cargo bench`
- [ ] Swift tests pass (manual - see voxpdf-swift/TESTING.md)
- [ ] Performance targets met (see docs/performance.md)

### Documentation

- [ ] Update README.md with v0.1.0 status
- [ ] Update ROADMAP.md to mark Phase 1 complete
- [ ] Create CHANGELOG.md entry for v0.1.0
- [ ] Document any deviations from original plan

### Git Hygiene

- [ ] All commits have descriptive messages
- [ ] No large files committed accidentally
- [ ] No sensitive data in commits
- [ ] Branch is up to date with main

---

## Troubleshooting

### Slice 0 Fails (lopdf insufficient)

**Problem:** lopdf cannot extract word positions

**Solution:**
1. Update `Cargo.toml` to use `mupdf-sys`
2. Rewrite `src/extraction/words.rs` using MuPDF APIs
3. Document decision in `docs/decisions/001-word-extraction-library.md`

### Swift Tests Fail

**Problem:** Cannot link Rust library

**Solution:**
1. Verify Rust library built: `ls target/release/libvoxpdf_core.dylib`
2. Add library path to Xcode build settings
3. See `voxpdf-swift/TESTING.md` for details

### Performance Below Target

**Problem:** Extraction takes >100ms per page

**Solution:**
1. Run profiler: `cargo instruments --release`
2. Identify hot paths
3. Consider caching parsed content streams
4. May need to optimize regex patterns

---

## Next Steps After v0.1.0

Once all slices complete and tests pass:

1. Create pull request from `feature/v0.1.0-implementation` to `main`
2. Code review
3. Merge to main
4. Tag release: `git tag v0.1.0`
5. Begin planning v0.2.0 (multi-column layouts)

---

**Plan Status:** Complete and ready for execution
**Last Updated:** 2025-11-07
**Target Completion:** 3 weeks from start
